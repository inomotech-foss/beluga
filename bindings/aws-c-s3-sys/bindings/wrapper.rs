/* automatically generated by rust-bindgen 0.69.1 */

pub type aws_s3_errors = ::core::ffi::c_uint;
pub type aws_s3_subject = ::core::ffi::c_uint;
#[doc = " A Meta Request represents a group of generated requests that are being done on behalf of the\n original request. For example, one large GetObject request can be transformed into a series\n of ranged GetObject requests that are executed in parallel to improve throughput.\n\n The aws_s3_meta_request_type is a hint of transformation to be applied."]
pub type aws_s3_meta_request_type = ::core::ffi::c_uint;
#[doc = " The type of a single S3 HTTP request. Used by metrics.\n A meta-request can make multiple S3 HTTP requests under the hood.\n\n For example, AWS_S3_META_REQUEST_TYPE_PUT_OBJECT for a large file will\n do multipart upload, resulting in 3+ HTTP requests:\n AWS_S3_REQUEST_TYPE_CREATE_MULTIPART_UPLOAD, one or more AWS_S3_REQUEST_TYPE_UPLOAD_PART,\n and finally AWS_S3_REQUEST_TYPE_COMPLETE_MULTIPART_UPLOAD.\n\n aws_s3_request_type_operation_name() returns the S3 operation name\n for types that map (e.g. AWS_S3_REQUEST_TYPE_HEAD_OBJECT -> \"HeadObject\"),\n or empty string for types that don't map (e.g. AWS_S3_REQUEST_TYPE_UNKNOWN -> \"\")."]
pub type aws_s3_request_type = ::core::ffi::c_uint;
#[doc = " Invoked to provide response headers received during execution of the meta request, both for\n success and error HTTP status codes.\n\n Return AWS_OP_SUCCESS to continue processing the request.\n\n Return aws_raise_error(E) to cancel the request.\n The error you raise will be reflected in `aws_s3_meta_request_result.error_code`.\n If you're not sure which error to raise, use AWS_ERROR_S3_CANCELED."]
pub type aws_s3_meta_request_headers_callback_fn = ::core::option::Option<
    unsafe extern "C" fn(
        meta_request: *mut aws_s3_meta_request,
        headers: *const aws_http_headers,
        response_status: ::core::ffi::c_int,
        user_data: *mut ::core::ffi::c_void,
    ) -> ::core::ffi::c_int,
>;
#[doc = " Invoked to provide the response body as it is received.\n\n Note: If you set `enable_read_backpressure` true on the S3 client,\n you must maintain the flow-control window.\n The flow-control window shrinks as you receive body data via this callback.\n Whenever the flow-control window reaches 0 you will stop downloading data.\n Use aws_s3_meta_request_increment_read_window() to increment the window and keep data flowing.\n Maintain a larger window to keep up a high download throughput,\n parts cannot download in parallel unless the window is large enough to hold multiple parts.\n Maintain a smaller window to limit the amount of data buffered in memory.\n\n If `manual_window_management` is false, you do not need to maintain the flow-control window.\n No back-pressure is applied and data arrives as fast as possible.\n\n Return AWS_OP_SUCCESS to continue processing the request.\n\n Return aws_raise_error(E) to cancel the request.\n The error you raise will be reflected in `aws_s3_meta_request_result.error_code`.\n If you're not sure which error to raise, use AWS_ERROR_S3_CANCELED."]
pub type aws_s3_meta_request_receive_body_callback_fn = ::core::option::Option<
    unsafe extern "C" fn(
        meta_request: *mut aws_s3_meta_request,
        body: *const aws_byte_cursor,
        range_start: u64,
        user_data: *mut ::core::ffi::c_void,
    ) -> ::core::ffi::c_int,
>;
#[doc = " Invoked when the entire meta request execution is complete."]
pub type aws_s3_meta_request_finish_fn = ::core::option::Option<
    unsafe extern "C" fn(
        meta_request: *mut aws_s3_meta_request,
        meta_request_result: *const aws_s3_meta_request_result,
        user_data: *mut ::core::ffi::c_void,
    ),
>;
#[doc = " Invoked to report progress of a meta-request.\n For PutObject, progress refers to bytes uploaded.\n For CopyObject, progress refers to bytes copied.\n For GetObject, progress refers to bytes downloaded.\n For anything else, progress refers to response body bytes received."]
pub type aws_s3_meta_request_progress_fn = ::core::option::Option<
    unsafe extern "C" fn(
        meta_request: *mut aws_s3_meta_request,
        progress: *const aws_s3_meta_request_progress,
        user_data: *mut ::core::ffi::c_void,
    ),
>;
#[doc = " Invoked to report the telemetry of the meta request once a single request finishes.\n Note: *metrics is only valid for the duration of the callback. If you need to keep it around, use\n `aws_s3_request_metrics_acquire`"]
pub type aws_s3_meta_request_telemetry_fn = ::core::option::Option<
    unsafe extern "C" fn(
        meta_request: *mut aws_s3_meta_request,
        metrics: *mut aws_s3_request_metrics,
        user_data: *mut ::core::ffi::c_void,
    ),
>;
pub type aws_s3_meta_request_shutdown_fn =
    ::core::option::Option<unsafe extern "C" fn(user_data: *mut ::core::ffi::c_void)>;
pub type aws_s3_client_shutdown_complete_callback_fn =
    ::core::option::Option<unsafe extern "C" fn(user_data: *mut ::core::ffi::c_void)>;
pub type aws_s3_meta_request_tls_mode = ::core::ffi::c_uint;
pub type aws_s3_meta_request_compute_content_md5 = ::core::ffi::c_uint;
pub type aws_s3_checksum_algorithm = ::core::ffi::c_uint;
pub type aws_s3_checksum_location = ::core::ffi::c_uint;
#[doc = " Optional callback, for you to review an upload before it completes.\n For example, you can review each part's checksum and fail the upload if\n you do not agree with them.\n\n @param meta_request pointer to the aws_s3_meta_request of the upload.\n @param info Detailed info about the upload.\n\n Return AWS_OP_SUCCESS to continue processing the request.\n\n Return aws_raise_error(E) to cancel the request.\n The error you raise will be reflected in `aws_s3_meta_request_result.error_code`.\n If you're not sure which error to raise, use AWS_ERROR_S3_CANCELED.\n\n WARNING: This feature is experimental/unstable.\n At this time, the callback is only invoked for multipart upload\n (when Content-Length is above the `multipart_upload_threshold`,\n or Content-Length not specified)."]
pub type aws_s3_meta_request_upload_review_fn = ::core::option::Option<
    unsafe extern "C" fn(
        meta_request: *mut aws_s3_meta_request,
        review: *const aws_s3_upload_review,
        user_data: *mut ::core::ffi::c_void,
    ) -> ::core::ffi::c_int,
>;
#[doc = " The factory function for S3 client to create a S3 Express credentials provider.\n The S3 client will be the only owner of the S3 Express credentials provider.\n\n During S3 client destruction, S3 client will start the destruction of the provider, and wait the\n on_provider_shutdown_callback to be invoked before the S3 client finish destruction.\n\n Note to implement the factory properly:\n - Make sure `on_provider_shutdown_callback` will be invoked after the provider finish shutdown, otherwise,\n leak will happen.\n - The provider must not acquire a reference to the client; otherwise, a circular reference will cause a deadlock.\n - The `client` provided CANNOT be used within the factory function call or the destructor.\n\n @param allocator    memory allocator to create the provider.\n @param client    The S3 client uses and owns the provider.\n @param on_provider_shutdown_callback    The callback to be invoked when the provider finishes shutdown.\n @param shutdown_user_data    The user data to invoke shutdown callback with\n @param user_data    The user data with the factory\n\n @return The aws_s3express_credentials_provider."]
pub type aws_s3express_provider_factory_fn = ::core::option::Option<
    unsafe extern "C" fn(
        allocator: *mut aws_allocator,
        client: *mut aws_s3_client,
        on_provider_shutdown_callback: aws_simple_completion_callback,
        shutdown_user_data: *mut ::core::ffi::c_void,
        factory_user_data: *mut ::core::ffi::c_void,
    ) -> *mut aws_s3express_credentials_provider,
>;
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_cpu_group_info {
    pub cpu_group: u16,
    pub nic_name_array: *mut aws_byte_cursor,
    pub nic_name_array_length: usize,
    pub cpus_in_group: usize,
}
#[repr(C)]
pub struct aws_s3_platform_info {
    pub instance_type: aws_byte_cursor,
    pub max_throughput_gbps: f64,
    pub cpu_group_info_array: *mut aws_s3_cpu_group_info,
    pub cpu_group_info_array_length: usize,
    pub has_recommended_configuration: bool,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_client {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_request {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_meta_request {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_meta_request_resume_token {
    _unused: [u8; 0],
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_request_metrics {
    _unused: [u8; 0],
}
#[doc = " Information sent in the meta_request progress callback."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_meta_request_progress {
    pub bytes_transferred: u64,
    pub content_length: u64,
}
#[doc = " Info about a single part, for you to review before the upload completes."]
#[repr(C)]
pub struct aws_s3_upload_part_review {
    pub size: u64,
    pub checksum: aws_byte_cursor,
}
#[doc = " Info for you to review before an upload completes.\n\n WARNING: This feature is experimental/unstable.\n At this time, review is only available for multipart upload\n (when Content-Length is above the `multipart_upload_threshold`,\n or Content-Length not specified)."]
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_upload_review {
    pub checksum_algorithm: aws_s3_checksum_algorithm,
    pub part_count: usize,
    pub part_array: *mut aws_s3_upload_part_review,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_tcp_keep_alive_options {
    pub keep_alive_interval_sec: u16,
    pub keep_alive_timeout_sec: u16,
    pub keep_alive_max_failed_probes: u16,
}
#[repr(C)]
pub struct aws_s3_client_config {
    pub max_active_connections_override: u32,
    pub region: aws_byte_cursor,
    pub client_bootstrap: *mut aws_client_bootstrap,
    pub tls_mode: aws_s3_meta_request_tls_mode,
    pub tls_connection_options: *mut aws_tls_connection_options,
    #[doc = " Required.\n Configure the signing for the requests made from the client.\n - Credentials or credentials provider is required. Other configs are all optional, and will be default to what\n      needs to sign the request for S3, only overrides when Non-zero/Not-empty is set.\n - To skip signing, you can config it with anonymous credentials.\n - S3 Client will derive the right config for signing process based on this.\n\n Notes:\n - For AWS_SIGNING_ALGORITHM_V4_S3EXPRESS, S3 client will use the credentials in the config to derive the\n S3 Express credentials that are used in the signing process.\n - For other auth algorithm, client may make modifications to signing config before passing it on to signer.\n\n TODO: deprecate this structure from auth, introduce a new S3 specific one."]
    pub signing_config: *mut aws_signing_config_aws,
    #[doc = " Optional.\n Size of parts the object will be downloaded or uploaded in, in bytes.\n This only affects AWS_S3_META_REQUEST_TYPE_GET_OBJECT and AWS_S3_META_REQUEST_TYPE_PUT_OBJECT.\n If not set, this defaults to 8 MiB.\n The client will adjust the part size for AWS_S3_META_REQUEST_TYPE_PUT_OBJECT if needed for service limits (max\n number of parts per upload is 10,000, minimum upload part size is 5 MiB).\n\n You can also set this per meta-request, via `aws_s3_meta_request_options.part_size`."]
    pub part_size: u64,
    pub max_part_size: u64,
    #[doc = " Optional.\n The size threshold in bytes for when to use multipart uploads.\n Uploads larger than this will use the multipart upload strategy.\n Uploads smaller or equal to this will use a single HTTP request.\n This only affects AWS_S3_META_REQUEST_TYPE_PUT_OBJECT.\n If set, this should be at least `part_size`.\n If not set, maximal of `part_size` and 5 MiB will be used.\n\n You can also set this per meta-request, via `aws_s3_meta_request_options.multipart_upload_threshold`."]
    pub multipart_upload_threshold: u64,
    pub throughput_target_gbps: f64,
    pub memory_limit_in_bytes: usize,
    pub retry_strategy: *mut aws_retry_strategy,
    #[doc = " TODO: move MD5 config to checksum config.\n For multi-part upload, content-md5 will be calculated if the AWS_MR_CONTENT_MD5_ENABLED is specified\n     or initial request has content-md5 header.\n For single-part upload, keep the content-md5 in the initial request unchanged."]
    pub compute_content_md5: aws_s3_meta_request_compute_content_md5,
    pub shutdown_callback: aws_s3_client_shutdown_complete_callback_fn,
    pub shutdown_callback_user_data: *mut ::core::ffi::c_void,
    #[doc = " Optional.\n Proxy configuration for http connection.\n If the connection_type is AWS_HPCT_HTTP_LEGACY, it will be converted to AWS_HPCT_HTTP_TUNNEL if tls_mode is\n ENABLED. Otherwise, it will be converted to AWS_HPCT_HTTP_FORWARD."]
    pub proxy_options: *mut aws_http_proxy_options,
    #[doc = " Optional.\n Configuration for fetching proxy configuration from environment.\n By Default proxy_ev_settings.aws_http_proxy_env_var_type is set to AWS_HPEV_ENABLE which means read proxy\n configuration from environment.\n Only works when proxy_options is not set. If both are set, configuration from proxy_options is used."]
    pub proxy_ev_settings: *mut proxy_env_var_settings,
    #[doc = " Optional.\n If set to 0, default value is used."]
    pub connect_timeout_ms: u32,
    #[doc = " Optional.\n Set keepalive to periodically transmit messages for detecting a disconnected peer."]
    pub tcp_keep_alive_options: *mut aws_s3_tcp_keep_alive_options,
    #[doc = " Optional.\n Configuration options for connection monitoring.\n If the transfer speed falls below the specified minimum_throughput_bytes_per_second, the operation is aborted.\n If set to NULL, default values are used."]
    pub monitoring_options: *mut aws_http_connection_monitoring_options,
    #[doc = " Enable backpressure and prevent response data from downloading faster than you can handle it.\n\n If false (default), no backpressure is applied and data will download as fast as possible.\n\n If true, each meta request has a flow-control window that shrinks as\n response body data is downloaded (headers do not affect the window).\n `initial_read_window` determines the starting size of each meta request's window.\n You will stop downloading data whenever the flow-control window reaches 0\n You must call aws_s3_meta_request_increment_read_window() to keep data flowing.\n\n WARNING: This feature is experimental.\n Currently, backpressure is only applied to GetObject requests which are split into multiple parts,\n and you may still receive some data after the window reaches 0."]
    pub enable_read_backpressure: bool,
    #[doc = " The starting size of each meta request's flow-control window, in bytes.\n Ignored unless `enable_read_backpressure` is true."]
    pub initial_read_window: usize,
    #[doc = " To enable S3 Express support or not."]
    pub enable_s3express: bool,
    #[doc = " Optional.\n Only used when `enable_s3express` is set.\n\n If set, client will invoke the factory to get the provider to use, when needed.\n\n If not set, client will create a default S3 Express provider under the hood."]
    pub s3express_provider_override_factory: aws_s3express_provider_factory_fn,
    pub factory_user_data: *mut ::core::ffi::c_void,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_checksum_config {
    #[doc = " The location of client added checksum header.\n\n If AWS_SCL_NONE. No request payload checksum will be add and calculated.\n\n If AWS_SCL_HEADER, the checksum will be calculated by client and added related header to the request sent.\n\n If AWS_SCL_TRAILER, the payload will be aws_chunked encoded, The checksum will be calculate while reading the\n payload by client. Related header will be added to the trailer part of the encoded payload. Note the payload of\n the original request cannot be aws-chunked encoded already. Otherwise, error will be raised."]
    pub location: aws_s3_checksum_location,
    #[doc = " The checksum algorithm used.\n Must be set if location is not AWS_SCL_NONE. Must be AWS_SCA_NONE if location is AWS_SCL_NONE."]
    pub checksum_algorithm: aws_s3_checksum_algorithm,
    #[doc = " Enable checksum mode header will be attached to GET requests, this will tell s3 to send back checksums headers if\n they exist. Calculate the corresponding checksum on the response bodies. The meta request will finish with a did\n validate field and set the error code to AWS_ERROR_S3_RESPONSE_CHECKSUM_MISMATCH if the calculated\n checksum, and checksum found in the response header do not match."]
    pub validate_response_checksum: bool,
    #[doc = " Optional array of `enum aws_s3_checksum_algorithm`.\n\n Ignored when validate_response_checksum is not set.\n If not set all the algorithms will be selected as default behavior.\n Owned by the caller.\n\n The list of algorithms for user to pick up when validate the checksum. Client will pick up the algorithm from the\n list with the priority based on performance, and the algorithm sent by server. The priority based on performance\n is [CRC32C, CRC32, SHA1, SHA256].\n\n If the response checksum was validated by client, the result will indicate which algorithm was picked."]
    pub validate_checksum_algorithms: *mut aws_array_list,
}
#[doc = " Options for a new meta request, ie, file transfer that will be handled by the high performance client.\n\n There are several ways to pass the request's body data:\n 1) If the data is already in memory, set the body-stream on `message`.\n 2) If the data is on disk, set `send_filepath` for best performance.\n 3) If the data will be be produced in asynchronous chunks, set `send_async_stream`."]
#[repr(C)]
pub struct aws_s3_meta_request_options {
    pub type_: aws_s3_meta_request_type,
    #[doc = " Optional.\n The S3 operation name (e.g. \"CreateBucket\").\n This will only be used when type is AWS_S3_META_REQUEST_TYPE_DEFAULT;\n it is automatically populated for other meta-request types.\n This name is used to fill out details in metrics and error reports."]
    pub operation_name: aws_byte_cursor,
    #[doc = " Configure the signing for each request created for this meta request. If NULL, options in the client will be\n  used.\n - The credentials will be obtained based on the precedence of:\n      1. `credentials` from `signing_config` in `aws_s3_meta_request_options`\n      2. `credentials_provider` from `signing_config` in `aws_s3_meta_request_options`\n      3. `credentials` from `signing_config` cached in the client\n      4. `credentials_provider` cached in the client\n - To skip signing, you can config it with anonymous credentials.\n - S3 Client will derive the right config for signing process based on this.\n\n Notes:\n - For AWS_SIGNING_ALGORITHM_V4_S3EXPRESS, S3 client will use the credentials in the config to derive the\n S3 Express credentials that are used in the signing process.\n - For other auth algorithm, client may make modifications to signing config before passing it on to signer."]
    pub signing_config: *const aws_signing_config_aws,
    pub message: *mut aws_http_message,
    #[doc = " Optional.\n If set, this file is sent as the request body.\n This gives the best performance when sending data from a file.\n Do not set if the body is being passed by other means (see note above)."]
    pub send_filepath: aws_byte_cursor,
    #[doc = " Optional - EXPERIMENTAL/UNSTABLE\n If set, the request body comes from this async stream.\n Use this when outgoing data will be produced in asynchronous chunks.\n Do not set if the body is being passed by other means (see note above)."]
    pub send_async_stream: *mut aws_async_input_stream,
    #[doc = " Optional.\n if set, the flexible checksum will be performed by client based on the config."]
    pub checksum_config: *const aws_s3_checksum_config,
    #[doc = " Optional.\n Size of parts the object will be downloaded or uploaded in, in bytes.\n This only affects AWS_S3_META_REQUEST_TYPE_GET_OBJECT and AWS_S3_META_REQUEST_TYPE_PUT_OBJECT.\n If not set, the value from `aws_s3_client_config.part_size` is used, which defaults to 8MiB.\n\n The client will adjust the part size for AWS_S3_META_REQUEST_TYPE_PUT_OBJECT if needed for service limits (max\n number of parts per upload is 10,000, minimum upload part size is 5 MiB)."]
    pub part_size: u64,
    #[doc = " Optional.\n The size threshold in bytes for when to use multipart uploads.\n Uploads larger than this will use the multipart upload strategy.\n Uploads smaller or equal to this will use a single HTTP request.\n This only affects AWS_S3_META_REQUEST_TYPE_PUT_OBJECT.\n If set, this should be at least `part_size`.\n If not set, `part_size` adjusted by client will be used as the threshold.\n If both `part_size` and `multipart_upload_threshold` are not set,\n the values from `aws_s3_client_config` are used."]
    pub multipart_upload_threshold: u64,
    pub user_data: *mut ::core::ffi::c_void,
    #[doc = " Optional.\n Invoked to provide response headers received during execution of the meta request.\n Note: this callback will not be fired for cases when resuming an\n operation that was already completed (ex. pausing put object after it\n uploaded all data and then resuming it)\n See `aws_s3_meta_request_headers_callback_fn`."]
    pub headers_callback: aws_s3_meta_request_headers_callback_fn,
    #[doc = " Invoked to provide the response body as it is received.\n See `aws_s3_meta_request_receive_body_callback_fn`."]
    pub body_callback: aws_s3_meta_request_receive_body_callback_fn,
    #[doc = " Invoked when the entire meta request execution is complete.\n See `aws_s3_meta_request_finish_fn`."]
    pub finish_callback: aws_s3_meta_request_finish_fn,
    pub shutdown_callback: aws_s3_meta_request_shutdown_fn,
    #[doc = " Invoked to report progress of the meta request execution.\n See `aws_s3_meta_request_progress_fn`."]
    pub progress_callback: aws_s3_meta_request_progress_fn,
    #[doc = " Optional.\n To get telemetry metrics when a single request finishes.\n If set the request will keep track of the metrics from `aws_s3_request_metrics`, and fire the callback when the\n request finishes receiving response.\n See `aws_s3_meta_request_telemetry_fn`"]
    pub telemetry_callback: aws_s3_meta_request_telemetry_fn,
    #[doc = " Optional.\n Callback for reviewing an upload before it completes.\n WARNING: experimental/unstable\n See `aws_s3_upload_review_fn`"]
    pub upload_review_callback: aws_s3_meta_request_upload_review_fn,
    #[doc = " Optional.\n Endpoint override for request. Can be used to override scheme and port of\n the endpoint.\n There is some overlap between Host header and Endpoint and corner cases\n are handled as follows:\n - Only Host header is set - Host is used to construct endpoint. https is\n   default with corresponding port\n - Only endpoint is set - Host header is created from endpoint. Port and\n   Scheme from endpoint is used.\n - Both Host and Endpoint is set - Host header must match Authority of\n   Endpoint uri. Port and Scheme from endpoint is used."]
    pub endpoint: *mut aws_uri,
    #[doc = " Optional.\n For meta requests that support pause/resume (e.g. PutObject), serialized resume token returned by\n aws_s3_meta_request_pause() can be provided here.\n Note: If PutObject request specifies a checksum algorithm, client will calculate checksums while skipping parts\n from the buffer and compare them them to previously uploaded part checksums."]
    pub resume_token: *mut aws_s3_meta_request_resume_token,
    pub object_size_hint: *mut u64,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3_meta_request_result {
    pub error_response_headers: *mut aws_http_headers,
    pub error_response_body: *mut aws_byte_buf,
    pub error_response_operation_name: *mut aws_string,
    pub response_status: ::core::ffi::c_int,
    pub did_validate: bool,
    pub validation_algorithm: aws_s3_checksum_algorithm,
    pub error_code: ::core::ffi::c_int,
}
#[repr(C)]
pub struct aws_s3_upload_resume_token_options {
    pub upload_id: aws_byte_cursor,
    pub part_size: u64,
    pub total_num_parts: usize,
    #[doc = " Optional.\n\n Note: during resume num_parts_uploaded is used for sanity checking against\n uploads on s3 side.\n In cases where upload id does not exist (already resumed using this token\n or pause called after upload completes, etc...) and num_parts_uploaded\n equals to total num parts, resume will become a noop."]
    pub num_parts_completed: usize,
}
#[repr(C)]
pub struct aws_credentials_properties_s3express {
    #[doc = " Required.\n The host address of the s3 bucket for the request."]
    pub host: aws_byte_cursor,
    #[doc = " Optional.\n The region of the bucket.\n If empty, the region of the S3 client will be used."]
    pub region: aws_byte_cursor,
}
#[repr(C)]
#[derive(Debug, Copy, Clone)]
pub struct aws_s3express_credentials_provider_vtable {
    #[doc = " Implementation for S3 Express provider to get S3 Express credentials"]
    pub get_credentials: ::core::option::Option<
        unsafe extern "C" fn(
            provider: *mut aws_s3express_credentials_provider,
            original_credentials: *const aws_credentials,
            properties: *const aws_credentials_properties_s3express,
            callback: aws_on_get_credentials_callback_fn,
            user_data: *mut ::core::ffi::c_void,
        ) -> ::core::ffi::c_int,
    >,
    #[doc = " Implementation to destroy the provider."]
    pub destroy: ::core::option::Option<
        unsafe extern "C" fn(provider: *mut aws_s3express_credentials_provider),
    >,
}
#[repr(C)]
pub struct aws_s3express_credentials_provider {
    pub vtable: *mut aws_s3express_credentials_provider_vtable,
    pub allocator: *mut aws_allocator,
    pub shutdown_complete_callback: aws_simple_completion_callback,
    pub shutdown_user_data: *mut ::core::ffi::c_void,
    pub impl_: *mut ::core::ffi::c_void,
    pub ref_count: aws_ref_count,
}
pub const AWS_C_S3_PACKAGE_ID: u32 = 14;
pub const AWS_ERROR_S3_MISSING_CONTENT_RANGE_HEADER: aws_s3_errors = 14336;
pub const AWS_ERROR_S3_INVALID_CONTENT_RANGE_HEADER: aws_s3_errors = 14337;
pub const AWS_ERROR_S3_MISSING_CONTENT_LENGTH_HEADER: aws_s3_errors = 14338;
pub const AWS_ERROR_S3_INVALID_CONTENT_LENGTH_HEADER: aws_s3_errors = 14339;
pub const AWS_ERROR_S3_MISSING_ETAG: aws_s3_errors = 14340;
pub const AWS_ERROR_S3_INTERNAL_ERROR: aws_s3_errors = 14341;
pub const AWS_ERROR_S3_SLOW_DOWN: aws_s3_errors = 14342;
pub const AWS_ERROR_S3_INVALID_RESPONSE_STATUS: aws_s3_errors = 14343;
pub const AWS_ERROR_S3_MISSING_UPLOAD_ID: aws_s3_errors = 14344;
pub const AWS_ERROR_S3_PROXY_PARSE_FAILED: aws_s3_errors = 14345;
pub const AWS_ERROR_S3_UNSUPPORTED_PROXY_SCHEME: aws_s3_errors = 14346;
pub const AWS_ERROR_S3_CANCELED: aws_s3_errors = 14347;
pub const AWS_ERROR_S3_INVALID_RANGE_HEADER: aws_s3_errors = 14348;
pub const AWS_ERROR_S3_MULTIRANGE_HEADER_UNSUPPORTED: aws_s3_errors = 14349;
pub const AWS_ERROR_S3_RESPONSE_CHECKSUM_MISMATCH: aws_s3_errors = 14350;
pub const AWS_ERROR_S3_CHECKSUM_CALCULATION_FAILED: aws_s3_errors = 14351;
pub const AWS_ERROR_S3_PAUSED: aws_s3_errors = 14352;
pub const AWS_ERROR_S3_LIST_PARTS_PARSE_FAILED: aws_s3_errors = 14353;
pub const AWS_ERROR_S3_RESUMED_PART_CHECKSUM_MISMATCH: aws_s3_errors = 14354;
pub const AWS_ERROR_S3_RESUME_FAILED: aws_s3_errors = 14355;
pub const AWS_ERROR_S3_OBJECT_MODIFIED: aws_s3_errors = 14356;
pub const AWS_ERROR_S3_NON_RECOVERABLE_ASYNC_ERROR: aws_s3_errors = 14357;
pub const AWS_ERROR_S3_METRIC_DATA_NOT_AVAILABLE: aws_s3_errors = 14358;
pub const AWS_ERROR_S3_INCORRECT_CONTENT_LENGTH: aws_s3_errors = 14359;
pub const AWS_ERROR_S3_REQUEST_TIME_TOO_SKEWED: aws_s3_errors = 14360;
pub const AWS_ERROR_S3_FILE_MODIFIED: aws_s3_errors = 14361;
pub const AWS_ERROR_S3_EXCEEDS_MEMORY_LIMIT: aws_s3_errors = 14362;
pub const AWS_ERROR_S3_INVALID_MEMORY_LIMIT_CONFIG: aws_s3_errors = 14363;
pub const AWS_ERROR_S3EXPRESS_CREATE_SESSION_FAILED: aws_s3_errors = 14364;
pub const AWS_ERROR_S3_INTERNAL_PART_SIZE_MISMATCH_RETRYING_WITH_RANGE: aws_s3_errors = 14365;
pub const AWS_ERROR_S3_END_RANGE: aws_s3_errors = 15359;
pub const AWS_LS_S3_GENERAL: aws_s3_subject = 14336;
pub const AWS_LS_S3_CLIENT: aws_s3_subject = 14337;
pub const AWS_LS_S3_CLIENT_STATS: aws_s3_subject = 14338;
pub const AWS_LS_S3_REQUEST: aws_s3_subject = 14339;
pub const AWS_LS_S3_META_REQUEST: aws_s3_subject = 14340;
pub const AWS_LS_S3_ENDPOINT: aws_s3_subject = 14341;
pub const AWS_LS_S3_LAST: aws_s3_subject = 15359;
#[doc = " The Default meta request type sends any request to S3 as-is (with no transformation). For example,\n it can be used to pass a CreateBucket request."]
pub const AWS_S3_META_REQUEST_TYPE_DEFAULT: aws_s3_meta_request_type = 0;
#[doc = " The GetObject request will be split into a series of ranged GetObject requests that are\n executed in parallel to improve throughput, when possible."]
pub const AWS_S3_META_REQUEST_TYPE_GET_OBJECT: aws_s3_meta_request_type = 1;
#[doc = " The PutObject request will be split into MultiPart uploads that are executed in parallel\n to improve throughput, when possible.\n Note: put object supports both known and unknown body length. The client\n relies on Content-Length header to determine length of the body.\n Request with unknown content length are always sent using multipart\n upload regardless of final number of parts and do have the following limitations:\n - multipart threshold is ignored and all request are made through mpu,\n   even if they only need one part\n - pause/resume is not supported\n - meta request will throw error if checksum header is provider (due to\n   general limitation of checksum not being usable if meta request is\n   getting split)"]
pub const AWS_S3_META_REQUEST_TYPE_PUT_OBJECT: aws_s3_meta_request_type = 2;
#[doc = " The CopyObject meta request performs a multi-part copy\n using multiple S3 UploadPartCopy requests in parallel, or bypasses\n a CopyObject request to S3 if the object size is not large enough for\n a multipart upload.\n Note: copy support is still in development and has following limitations:\n - host header must use virtual host addressing style (path style is not\n   supported) and both source and dest buckets must have dns compliant name\n - only {bucket}/{key} format is supported for source and passing arn as\n   source will not work\n - source bucket is assumed to be in the same region as dest"]
pub const AWS_S3_META_REQUEST_TYPE_COPY_OBJECT: aws_s3_meta_request_type = 3;
#[doc = " The CopyObject meta request performs a multi-part copy\n using multiple S3 UploadPartCopy requests in parallel, or bypasses\n a CopyObject request to S3 if the object size is not large enough for\n a multipart upload.\n Note: copy support is still in development and has following limitations:\n - host header must use virtual host addressing style (path style is not\n   supported) and both source and dest buckets must have dns compliant name\n - only {bucket}/{key} format is supported for source and passing arn as\n   source will not work\n - source bucket is assumed to be in the same region as dest"]
pub const AWS_S3_META_REQUEST_TYPE_MAX: aws_s3_meta_request_type = 4;
pub const AWS_S3_REQUEST_TYPE_UNKNOWN: aws_s3_request_type = 0;
pub const AWS_S3_REQUEST_TYPE_HEAD_OBJECT: aws_s3_request_type = 1;
pub const AWS_S3_REQUEST_TYPE_GET_OBJECT: aws_s3_request_type = 2;
pub const AWS_S3_REQUEST_TYPE_LIST_PARTS: aws_s3_request_type = 3;
pub const AWS_S3_REQUEST_TYPE_CREATE_MULTIPART_UPLOAD: aws_s3_request_type = 4;
pub const AWS_S3_REQUEST_TYPE_UPLOAD_PART: aws_s3_request_type = 5;
pub const AWS_S3_REQUEST_TYPE_ABORT_MULTIPART_UPLOAD: aws_s3_request_type = 6;
pub const AWS_S3_REQUEST_TYPE_COMPLETE_MULTIPART_UPLOAD: aws_s3_request_type = 7;
pub const AWS_S3_REQUEST_TYPE_UPLOAD_PART_COPY: aws_s3_request_type = 8;
pub const AWS_S3_REQUEST_TYPE_COPY_OBJECT: aws_s3_request_type = 9;
pub const AWS_S3_REQUEST_TYPE_PUT_OBJECT: aws_s3_request_type = 10;
pub const AWS_S3_REQUEST_TYPE_MAX: aws_s3_request_type = 11;
#[doc = " @deprecated Use AWS_S3_REQUEST_TYPE_UNKNOWN if the actual S3 HTTP request type is unknown"]
pub const AWS_S3_REQUEST_TYPE_DEFAULT: aws_s3_request_type = 0;
pub const AWS_MR_TLS_ENABLED: aws_s3_meta_request_tls_mode = 0;
pub const AWS_MR_TLS_DISABLED: aws_s3_meta_request_tls_mode = 1;
pub const AWS_MR_CONTENT_MD5_DISABLED: aws_s3_meta_request_compute_content_md5 = 0;
pub const AWS_MR_CONTENT_MD5_ENABLED: aws_s3_meta_request_compute_content_md5 = 1;
pub const AWS_SCA_NONE: aws_s3_checksum_algorithm = 0;
pub const AWS_SCA_INIT: aws_s3_checksum_algorithm = 1;
pub const AWS_SCA_CRC32C: aws_s3_checksum_algorithm = 1;
pub const AWS_SCA_CRC32: aws_s3_checksum_algorithm = 2;
pub const AWS_SCA_SHA1: aws_s3_checksum_algorithm = 3;
pub const AWS_SCA_SHA256: aws_s3_checksum_algorithm = 4;
pub const AWS_SCA_END: aws_s3_checksum_algorithm = 4;
pub const AWS_SCL_NONE: aws_s3_checksum_location = 0;
pub const AWS_SCL_HEADER: aws_s3_checksum_location = 1;
pub const AWS_SCL_TRAILER: aws_s3_checksum_location = 2;
extern "C" {
    #[doc = " Initializes internal datastructures used by aws-c-s3.\n Must be called before using any functionality in aws-c-s3."]
    pub fn aws_s3_library_init(allocator: *mut aws_allocator);
    #[doc = " Shuts down the internal datastructures used by aws-c-s3."]
    pub fn aws_s3_library_clean_up();
    pub fn aws_s3_get_current_platform_info() -> *const aws_s3_platform_info;
    pub fn aws_s3_get_platforms_with_recommended_config() -> aws_array_list;
    pub fn aws_s3_client_new(
        allocator: *mut aws_allocator,
        client_config: *const aws_s3_client_config,
    ) -> *mut aws_s3_client;
    #[doc = " Add a reference, keeping this object alive.\n The reference must be released when you are done with it, or it's memory will never be cleaned up.\n You must not pass in NULL.\n Always returns the same pointer that was passed in."]
    pub fn aws_s3_client_acquire(client: *mut aws_s3_client) -> *mut aws_s3_client;
    #[doc = " Release a reference.\n When the reference count drops to 0, this object will be cleaned up.\n It's OK to pass in NULL (nothing happens).\n Always returns NULL."]
    pub fn aws_s3_client_release(client: *mut aws_s3_client) -> *mut aws_s3_client;
    pub fn aws_s3_client_make_meta_request(
        client: *mut aws_s3_client,
        options: *const aws_s3_meta_request_options,
    ) -> *mut aws_s3_meta_request;
    #[doc = " Increment the flow-control window, so that response data continues downloading.\n\n If the client was created with `enable_read_backpressure` set true,\n each meta request has a flow-control window that shrinks as response\n body data is downloaded (headers do not affect the size of the window).\n The client's `initial_read_window` determines the starting size of each meta request's window.\n If a meta request's flow-control window reaches 0, no further data will be downloaded.\n If the `initial_read_window` is 0, the request will not start until the window is incremented.\n Maintain a larger window to keep up a high download throughput,\n parts cannot download in parallel unless the window is large enough to hold multiple parts.\n Maintain a smaller window to limit the amount of data buffered in memory.\n\n If `enable_read_backpressure` is false this call will have no effect,\n no backpressure is being applied and data is being downloaded as fast as possible.\n\n WARNING: This feature is experimental.\n Currently, backpressure is only applied to GetObject requests which are split into multiple parts,\n and you may still receive some data after the window reaches 0."]
    pub fn aws_s3_meta_request_increment_read_window(
        meta_request: *mut aws_s3_meta_request,
        bytes: u64,
    );
    pub fn aws_s3_meta_request_cancel(meta_request: *mut aws_s3_meta_request);
    #[doc = " Note: pause is currently only supported on upload requests.\n In order to pause an ongoing upload, call aws_s3_meta_request_pause() that\n will return resume token. Token can be used to query the state of operation\n at the pausing time.\n To resume an upload that was paused, supply resume token in the meta\n request options structure member aws_s3_meta_request_options.resume_token.\n The upload can be resumed either from the same client or a different one.\n Corner cases for resume upload are as follows:\n - upload is not MPU - fail with AWS_ERROR_UNSUPPORTED_OPERATION\n - pausing before MPU is created - NULL resume token returned. NULL resume\n   token is equivalent to restarting upload\n - pausing in the middle of part transfer - return resume token. scheduling of\n   new part uploads stops.\n - pausing after completeMPU started - return resume token. if s3 cannot find\n   find associated MPU id when resuming with that token and num of parts\n   uploaded equals to total num parts, then operation is a no op. Otherwise\n   operation fails.\n Note: for no op case the call will succeed and finish/shutdown request callbacks will\n   fire, but on headers callback will not fire.\n Note: similar to cancel pause does not cancel requests already in flight and\n and parts might complete after pause is requested.\n @param meta_request pointer to the aws_s3_meta_request of the upload to be paused\n @param resume_token resume token\n @return either AWS_OP_ERR or AWS_OP_SUCCESS"]
    pub fn aws_s3_meta_request_pause(
        meta_request: *mut aws_s3_meta_request,
        out_resume_token: *mut *mut aws_s3_meta_request_resume_token,
    ) -> ::core::ffi::c_int;
    #[doc = " Create upload resume token from persisted data.\n Note: Data required for resume token varies per operation."]
    pub fn aws_s3_meta_request_resume_token_new_upload(
        allocator: *mut aws_allocator,
        options: *const aws_s3_upload_resume_token_options,
    ) -> *mut aws_s3_meta_request_resume_token;
    pub fn aws_s3_meta_request_resume_token_acquire(
        resume_token: *mut aws_s3_meta_request_resume_token,
    ) -> *mut aws_s3_meta_request_resume_token;
    pub fn aws_s3_meta_request_resume_token_release(
        resume_token: *mut aws_s3_meta_request_resume_token,
    ) -> *mut aws_s3_meta_request_resume_token;
    pub fn aws_s3_meta_request_resume_token_type(
        resume_token: *mut aws_s3_meta_request_resume_token,
    ) -> aws_s3_meta_request_type;
    pub fn aws_s3_meta_request_resume_token_part_size(
        resume_token: *mut aws_s3_meta_request_resume_token,
    ) -> u64;
    pub fn aws_s3_meta_request_resume_token_total_num_parts(
        resume_token: *mut aws_s3_meta_request_resume_token,
    ) -> usize;
    pub fn aws_s3_meta_request_resume_token_num_parts_completed(
        resume_token: *mut aws_s3_meta_request_resume_token,
    ) -> usize;
    pub fn aws_s3_meta_request_resume_token_upload_id(
        resume_token: *mut aws_s3_meta_request_resume_token,
    ) -> aws_byte_cursor;
    #[doc = " Add a reference, keeping this object alive.\n The reference must be released when you are done with it, or it's memory will never be cleaned up.\n You must not pass in NULL.\n Always returns the same pointer that was passed in."]
    pub fn aws_s3_meta_request_acquire(
        meta_request: *mut aws_s3_meta_request,
    ) -> *mut aws_s3_meta_request;
    #[doc = " Release a reference.\n When the reference count drops to 0, this object will be cleaned up.\n It's OK to pass in NULL (nothing happens).\n Always returns NULL."]
    pub fn aws_s3_meta_request_release(
        meta_request: *mut aws_s3_meta_request,
    ) -> *mut aws_s3_meta_request;
    #[doc = " Initialize the configuration for a default S3 signing."]
    pub fn aws_s3_init_default_signing_config(
        signing_config: *mut aws_signing_config_aws,
        region: aws_byte_cursor,
        credentials_provider: *mut aws_credentials_provider,
    );
    #[doc = " Return operation name for aws_s3_request_type,\n or empty string if the type doesn't map to an actual operation.\n For example:\n AWS_S3_REQUEST_TYPE_HEAD_OBJECT -> \"HeadObject\"\n AWS_S3_REQUEST_TYPE_UNKNOWN -> \"\"\n AWS_S3_REQUEST_TYPE_MAX -> \"\""]
    pub fn aws_s3_request_type_operation_name(
        type_: aws_s3_request_type,
    ) -> *const ::core::ffi::c_char;
    #[doc = " Add a reference, keeping this object alive.\n The reference must be released when you are done with it, or it's memory will never be cleaned up.\n Always returns the same pointer that was passed in."]
    pub fn aws_s3_request_metrics_acquire(
        metrics: *mut aws_s3_request_metrics,
    ) -> *mut aws_s3_request_metrics;
    #[doc = " Release a reference.\n When the reference count drops to 0, this object will be cleaned up.\n It's OK to pass in NULL (nothing happens).\n Always returns NULL."]
    pub fn aws_s3_request_metrics_release(
        metrics: *mut aws_s3_request_metrics,
    ) -> *mut aws_s3_request_metrics;
    #[doc = "  Getters for s3 request metrics ************************************************/\n/**\n Get the request ID from aws_s3_request_metrics.\n If unavailable, AWS_ERROR_S3_METRIC_DATA_NOT_AVAILABLE will be raised.\n If available, out_request_id will be set to a string. Be warned this string's lifetime is tied to the metrics\n object."]
    pub fn aws_s3_request_metrics_get_request_id(
        metrics: *const aws_s3_request_metrics,
        out_request_id: *mut *const aws_string,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_start_timestamp_ns(
        metrics: *const aws_s3_request_metrics,
        out_start_time: *mut u64,
    );
    pub fn aws_s3_request_metrics_get_end_timestamp_ns(
        metrics: *const aws_s3_request_metrics,
        out_end_time: *mut u64,
    );
    pub fn aws_s3_request_metrics_get_total_duration_ns(
        metrics: *const aws_s3_request_metrics,
        out_total_duration: *mut u64,
    );
    pub fn aws_s3_request_metrics_get_send_start_timestamp_ns(
        metrics: *const aws_s3_request_metrics,
        out_send_start_time: *mut u64,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_send_end_timestamp_ns(
        metrics: *const aws_s3_request_metrics,
        out_send_end_time: *mut u64,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_sending_duration_ns(
        metrics: *const aws_s3_request_metrics,
        out_sending_duration: *mut u64,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_receive_start_timestamp_ns(
        metrics: *const aws_s3_request_metrics,
        out_receive_start_time: *mut u64,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_receive_end_timestamp_ns(
        metrics: *const aws_s3_request_metrics,
        out_receive_end_time: *mut u64,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_receiving_duration_ns(
        metrics: *const aws_s3_request_metrics,
        out_receiving_duration: *mut u64,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_response_status_code(
        metrics: *const aws_s3_request_metrics,
        out_response_status: *mut ::core::ffi::c_int,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_response_headers(
        metrics: *const aws_s3_request_metrics,
        out_response_headers: *mut *mut aws_http_headers,
    ) -> ::core::ffi::c_int;
    #[doc = " Get the path and query of the request.\n If unavailable, AWS_ERROR_S3_METRIC_DATA_NOT_AVAILABLE will be raised.\n If available, out_request_path_query will be set to a string. Be warned this string's lifetime is tied to the metrics\n object."]
    pub fn aws_s3_request_metrics_get_request_path_query(
        metrics: *const aws_s3_request_metrics,
        out_request_path_query: *mut *const aws_string,
    );
    #[doc = " Get the host_address of the request.\n If unavailable, AWS_ERROR_S3_METRIC_DATA_NOT_AVAILABLE will be raised.\n If available, out_host_address will be set to a string. Be warned this string's lifetime is tied to the metrics\n object."]
    pub fn aws_s3_request_metrics_get_host_address(
        metrics: *const aws_s3_request_metrics,
        out_host_address: *mut *const aws_string,
    );
    #[doc = " Get the IP address of the request connected to.\n If unavailable, AWS_ERROR_S3_METRIC_DATA_NOT_AVAILABLE will be raised.\n If available, out_ip_address will be set to a string. Be warned this string's lifetime is tied to the metrics object."]
    pub fn aws_s3_request_metrics_get_ip_address(
        metrics: *const aws_s3_request_metrics,
        out_ip_address: *mut *const aws_string,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_connection_id(
        metrics: *const aws_s3_request_metrics,
        out_connection_id: *mut usize,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_thread_id(
        metrics: *const aws_s3_request_metrics,
        out_thread_id: *mut aws_thread_id_t,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_request_stream_id(
        metrics: *const aws_s3_request_metrics,
        out_stream_id: *mut u32,
    ) -> ::core::ffi::c_int;
    #[doc = " Get the S3 operation name of the request (e.g. \"HeadObject\").\n If unavailable, AWS_ERROR_S3_METRIC_DATA_NOT_AVAILABLE will be raised.\n If available, out_operation_name will be set to a string.\n Be warned this string's lifetime is tied to the metrics object."]
    pub fn aws_s3_request_metrics_get_operation_name(
        metrics: *const aws_s3_request_metrics,
        out_operation_name: *mut *const aws_string,
    ) -> ::core::ffi::c_int;
    pub fn aws_s3_request_metrics_get_request_type(
        metrics: *const aws_s3_request_metrics,
        out_request_type: *mut aws_s3_request_type,
    );
    pub fn aws_s3_request_metrics_get_error_code(
        metrics: *const aws_s3_request_metrics,
    ) -> ::core::ffi::c_int;
    #[doc = " Creates a new S3 endpoint resolver.\n Warning: Before using this header, you have to enable it by\n setting cmake config AWS_ENABLE_S3_ENDPOINT_RESOLVER=ON"]
    pub fn aws_s3_endpoint_resolver_new(
        allocator: *mut aws_allocator,
    ) -> *mut aws_endpoints_rule_engine;
    pub fn aws_s3express_credentials_provider_release(
        provider: *mut aws_s3express_credentials_provider,
    ) -> *mut aws_s3express_credentials_provider;
    #[doc = " To initialize the provider with basic vtable and refcount. And hook up the refcount with vtable functions.\n\n @param provider\n @param allocator\n @param vtable\n @param impl Optional, the impl for the provider\n @return AWS_S3_API"]
    pub fn aws_s3express_credentials_provider_init_base(
        provider: *mut aws_s3express_credentials_provider,
        allocator: *mut aws_allocator,
        vtable: *mut aws_s3express_credentials_provider_vtable,
        impl_: *mut ::core::ffi::c_void,
    );
    #[doc = " Async function for retrieving specific credentials based on properties.\n\n @param provider aws_s3express_credentials_provider provider to source from\n @param original_credentials The credentials used to derive the credentials for S3 Express.\n @param properties Specific properties for credentials being fetched.\n @param user_data user data to pass to the completion callback\n\n callback will only be invoked if-and-only-if the return value was AWS_OP_SUCCESS.\n"]
    pub fn aws_s3express_credentials_provider_get_credentials(
        provider: *mut aws_s3express_credentials_provider,
        original_credentials: *const aws_credentials,
        properties: *const aws_credentials_properties_s3express,
        callback: aws_on_get_credentials_callback_fn,
        user_data: *mut ::core::ffi::c_void,
    ) -> ::core::ffi::c_int;
}
